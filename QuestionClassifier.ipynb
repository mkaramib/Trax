{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuestionClassifier.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOYacTAseZJMwklJMBwzwzs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkaramib/trax/blob/main/QuestionClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT5D9NVs7jp9"
      },
      "source": [
        "## Question Classification\r\n",
        "In this notebook, I will implement a question classifier using Trax deep learning framework. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXrjGZy9aj2s"
      },
      "source": [
        "import numpy as np_base  # regular ol' numpy\r\n",
        "import os\r\n",
        "import random as rnd\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from unicodedata import normalize\r\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXeCE1qqhhEq"
      },
      "source": [
        "# Initialize\r\n",
        "Some of the libraries need to be downlowed or initialized such as NLTK tokenizer and stop-words. Following lines will do these steps. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJqnWqA5hwmu"
      },
      "source": [
        "# initialize\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH_6OpuCa-GI"
      },
      "source": [
        "# Trax\r\n",
        "In this section, we need to install [trax](https://github.com/google/trax) if it is not installed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykH9bNfx7iBh"
      },
      "source": [
        "!pip install -q -U trax\r\n",
        "import trax\r\n",
        "from trax import layers as tl  # core building block\r\n",
        "from trax import shapes  # data signatures: dimensionality and type\r\n",
        "from trax import fastmath  # uses jax, offers numpy on steroids\r\n",
        "from trax.supervised import training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NGCJvW7arR1"
      },
      "source": [
        "Check which version of Trax has been installed.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qT5kXjoaqk8"
      },
      "source": [
        "!pip list | grep trax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS3sJ73AboA7"
      },
      "source": [
        "## Trax Numpy\r\n",
        "The key mathematical benefit of Trax is using JAX to implement its numpy version. So, following line will import Trax' numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-g-O_-zb59u"
      },
      "source": [
        "# import trax.fastmath.numpy\r\n",
        "import trax.fastmath.numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hctu-A1i78ib"
      },
      "source": [
        "# Data Processing\r\n",
        "In this section, all the training and testing questions are read.\r\n",
        "*   Train Data: contains 1000, 2000, 3000, 4000, or 5500 questions in each file.\r\n",
        "*   Validation Data: we use one of training files for validation data.\r\n",
        "*   Test Data: contains close to 500 questions to evaluate the trained model.\r\n",
        "\r\n",
        "In each file(train and test), each line contains a question which has the following format:\r\n",
        "*   QuestionCategory: Question content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUukXdvH80o0"
      },
      "source": [
        "train_file = open(\"./questions/train_2000.label\", mode='r',encoding=\"ISO-8859-1\")\r\n",
        "val_file = open(\"./questions/train_1000.label\", mode='r',encoding=\"ISO-8859-1\")\r\n",
        "test_file = open(\"./questions/TREC_10.label\", mode='r', encoding=\"ISO-8859-1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXpxmzTQcTgR"
      },
      "source": [
        "## Tokenization\r\n",
        "One of the key steps in the preprocess is to tokenize the questions. In this experiment, we use NLTK tokenization. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf75I1FniKGu"
      },
      "source": [
        "def tokenize(question):\r\n",
        "    \"\"\"\r\n",
        "    separate the question type as well as question tokens\r\n",
        "    :param question: given question\r\n",
        "    :return: question_category, question_terms\r\n",
        "    \"\"\"\r\n",
        "    colon = question.find(':')            # index of first colon to separate the question category\r\n",
        "    q_cat = question[0:colon]             # get question type\r\n",
        "    content_normalized = normalize('NFKC', question[colon:])  # normalize the content\r\n",
        "    content_normalized = re.sub(\"[^a-zA-Z. ]\", \"\", content_normalized)  # remove non-alphabetic parts of question\r\n",
        "    terms_all = word_tokenize(content_normalized)             # tokenize the content\r\n",
        "\r\n",
        "    # remove the stop words\r\n",
        "    terms = [w for w in terms_all if w not in stop_words]\r\n",
        "    #terms = terms_all\r\n",
        "    return q_cat, terms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EWiTKFHjIKe"
      },
      "source": [
        "## Data Preprocess\r\n",
        "In this step, all the question are read, tokenized and stored in list of tuples: *(category, terms)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1fwqiASjg3k"
      },
      "source": [
        "def load_data(file):\r\n",
        "  \"\"\"\r\n",
        "  read the lines from the given file and prepare the list of tuples of questions.\r\n",
        "  :param file: given file\r\n",
        "  :return: list of tuples(category, terms)\r\n",
        "  \"\"\"\r\n",
        "  questions = []\r\n",
        "  lines = file.readlines()\r\n",
        "  for line in lines:\r\n",
        "    cat, terms = tokenize(line)\r\n",
        "    questions.append((cat, terms))\r\n",
        "  return questions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cwl6V3pju9sz"
      },
      "source": [
        "# test load_data\r\n",
        "temp_train_qs = load_data(train_file)\r\n",
        "print(f'number of questions for training = {len(temp_train_qs)}')\r\n",
        "print(f'as an example = {temp_train_qs[0]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3dprvM9lCJ-"
      },
      "source": [
        "## Vocabulary and Targets\r\n",
        "We need the vocabulary and targets to train. In this step, we will make them ready. In the future, we need to convert the content(question) to tensor which is list of numbers. So, we need to keep unique id for each term. We have covered this in the vocab dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri1HLlEglYml"
      },
      "source": [
        "def build_vocabulary(questions):\r\n",
        "  \"\"\"\r\n",
        "  Generate the vocabulary from the questions. \r\n",
        "  :param questions: given list of tuples(cat, terms)\r\n",
        "  :return: list of unique categories, vocabulary(dictionary of term:Id)\r\n",
        "  \"\"\"\r\n",
        "  cats = [cat for (cat, _) in questions]\r\n",
        "  vocab = {'__PAD__': 0, '__UNK__': 1}\r\n",
        "  for (_, terms) in questions:\r\n",
        "    for term in terms:\r\n",
        "       if term not in vocab:\r\n",
        "         vocab[term] = len(vocab) \r\n",
        "  return list(set(cats)), vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X-IsDLrwISE"
      },
      "source": [
        "# test the vocabulary builder\r\n",
        "temp_cats, temp_vocabs = build_vocabulary(temp_train_qs)\r\n",
        "print(f'Categories = {temp_cats}')\r\n",
        "print(f'number of terms in vocab = {len(temp_vocabs)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJv3tObDxPoV"
      },
      "source": [
        "## Build Tensor\r\n",
        "Oen of the first steps in training any neural network is to convert any input to tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SydeK4axht0"
      },
      "source": [
        "def question_to_tensor(question, vocab, unk_token=\"__UNK__\"):\r\n",
        "  \"\"\"\r\n",
        "  convert the given question into tensor\r\n",
        "  :param question: list of terms of question, [t1, t2, ...]\r\n",
        "  :param vocab: dictionary of vocabulary\r\n",
        "  :param unk_token: token to be used for the terms that are not in the vocabs.\r\n",
        "  :return: tensor = [1, 4, 2, ...]\r\n",
        "  \"\"\"\r\n",
        "  tensor = []\r\n",
        "  for term in question:\r\n",
        "    # get the id for the term\r\n",
        "    word_id = vocab[term] if term in vocab else vocab[unk_token]\r\n",
        "    tensor.append(word_id)\r\n",
        "\r\n",
        "  return tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PnYQsW9xAmM"
      },
      "source": [
        "# test question_to_tensor builder\r\n",
        "temp_tensor = question_to_tensor(temp_train_qs[1][1],temp_vocabs)\r\n",
        "print(f'question_terms = {temp_train_qs[1][1]}, tensro = {temp_tensor}, shape = {len(temp_tensor)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouohzLWtkuGB"
      },
      "source": [
        "## Data Loader\r\n",
        "It is required to load *train*, *validation*, and *test* data and collect the vocabulary and question categories. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqOWdh4blB-s"
      },
      "source": [
        "# load training questions\r\n",
        "training_data = load_data(train_file)\r\n",
        "\r\n",
        "# load validation questions\r\n",
        "validation_data = load_data(val_file)\r\n",
        "\r\n",
        "# load test questions\r\n",
        "test_data = load_data(test_file)\r\n",
        "\r\n",
        "# build vocabulary\r\n",
        "cats, vocab = build_vocabulary(training_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsZQBusK0NVQ"
      },
      "source": [
        "## Data Generator(Batch)\r\n",
        "In most Deep NN models, the inputs are given in batches. A batch generator is implemented to generate batches of data samples for *train*, *validation*, and *test*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG0yKxQO0oes"
      },
      "source": [
        "def data_generator(data, vocab, cats, batch_size, loop=False, shuffle=False):\r\n",
        "  '''\r\n",
        "  Generate a batch of samples from the given data.\r\n",
        "  :param data: list of tuples of questions:(cat, terms).\r\n",
        "  :param vocab: vocabulary dictionary {term:id, ...}\r\n",
        "  :param cats: list of categories of questions.\r\n",
        "  :param batch_size: size of batch.\r\n",
        "  :param loop: True/False to loop back at the end of data.\r\n",
        "  :param shuffle: True/False, shuffle the data or not.\r\n",
        "  :Yield: inputs: subset of data samples, target: corresponing targets of selected inputs.\r\n",
        "  '''\r\n",
        "\r\n",
        "  # build a list of indexes for data samples\r\n",
        "  data_l = len(data)\r\n",
        "  data_indexes = list(range(data_l))\r\n",
        "\r\n",
        "  # get the max length of questions for padding. \r\n",
        "  max_l = 0\r\n",
        "  for (_,q) in data:\r\n",
        "    max_l = max(max_l, len(q))\r\n",
        "\r\n",
        "  # shuffle the indexes if it is True\r\n",
        "  if shuffle:\r\n",
        "    rnd.shuffle(data_indexes)\r\n",
        "\r\n",
        "  stop = False\r\n",
        "  index = 0\r\n",
        "\r\n",
        "  while not stop:\r\n",
        "    batch = []\r\n",
        "    targets = []\r\n",
        "    \r\n",
        "    for i in range(batch_size):\r\n",
        "\r\n",
        "        # if at the end of data.\r\n",
        "        if index >= len(data_indexes):\r\n",
        "          if not loop:\r\n",
        "            stop = True\r\n",
        "            break\r\n",
        "          \r\n",
        "          # start index from 0\r\n",
        "          index = 0\r\n",
        "          \r\n",
        "          # shuffle the data indexes if required\r\n",
        "          if shuffle:\r\n",
        "            rnd.shuffle(data_indexes)\r\n",
        "          \r\n",
        "        # get the question, convert to tensor, and append the data and target\r\n",
        "        q = data[data_indexes[index]]\r\n",
        "        q_tensor = question_to_tensor(q[1], vocab)\r\n",
        "        \r\n",
        "        # pad the batched tensors to the longest question in the data.\r\n",
        "        q_tensor_pad = q_tensor + [vocab[\"__PAD__\"]]*(max_l - len(q_tensor)) \r\n",
        "        \r\n",
        "        batch.append(q_tensor_pad)\r\n",
        "        targets.append(cats.index(q[0]))\r\n",
        "\r\n",
        "        # increase index\r\n",
        "        index += 1\r\n",
        "\r\n",
        "    # yield the batch and targets\r\n",
        "    yield np.array(batch), np.array(targets)\r\n",
        "\r\n",
        "    # if stop\r\n",
        "    if stop:\r\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kfxAfJPyWcI"
      },
      "source": [
        "# test the data_generator\r\n",
        "temp_data_generator = data_generator(temp_train_qs, temp_vocabs, temp_cats, batch_size=8,loop=False)\r\n",
        "temp_next = next(temp_data_generator)\r\n",
        "print(f'batch shape = {temp_next[0]}, targets = {temp_next[1]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDUIDW-ZgrAp"
      },
      "source": [
        "We need to build data generators for training, validation, and testing processes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9QFzUnHhHmU"
      },
      "source": [
        "def train_generator(batch_size, shuffle=False):\r\n",
        "  return data_generator(train_questions, vocab, cats, batch_size, True,shuffle=False)\r\n",
        "\r\n",
        "def eval_generator(batch_size, shuffle=False):\r\n",
        "  return data_generator(eval_questions, vocab, cats, batch_size, True,shuffle=False)\r\n",
        "\r\n",
        "def test_generator(batch_size, shuffle=False):\r\n",
        "  return data_generator(test_questions, vocab, cats, batch_size, True,shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xkkVXRGy8RO"
      },
      "source": [
        "# Training\r\n",
        "Training consists of following steps:\r\n",
        "1.   Define the NN model\r\n",
        "2.   Define the training model\r\n",
        "3.   Instantiate the model and training loop\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWz1k5PD6hz4"
      },
      "source": [
        "## NN Model\r\n",
        "Classifier is a function that design the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn7hNf3B6u6x"
      },
      "source": [
        "def classifier(vocab_size=len(Vocab), embedding_dim=256, output_dim=2, mode='train'):\r\n",
        "    # create embedding layer\r\n",
        "    embed_layer = tl.Embedding(\r\n",
        "        vocab_size = vocab_size,    # Size of the vocabulary\r\n",
        "        d_feature = embedding_dim)  # Embedding dimension\r\n",
        "    \r\n",
        "    # Create a mean layer, to create an \"average\" word embedding\r\n",
        "    mean_layer = tl.Mean(axis=1)\r\n",
        "    \r\n",
        "    # Create a dense layer, one unit for each output\r\n",
        "    dense_output_layer = tl.Dense(n_units = output_dim)\r\n",
        "    \r\n",
        "    # Create the log softmax layer.\r\n",
        "    log_softmax_layer = tl.LogSoftmax()\r\n",
        "    \r\n",
        "    # Create a model using tl.Serial to combine all layers\r\n",
        "    model = tl.Serial(\r\n",
        "      embed_layer,        # embedding layer\r\n",
        "      mean_layer,         # mean layer\r\n",
        "      dense_output_layer, # dense output layer \r\n",
        "      log_softmax_layer   # log softmax layer\r\n",
        "    )\r\n",
        "\r\n",
        "    # return the model of type\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Eax3EJ7v1a"
      },
      "source": [
        "## Training Model\r\n",
        "Training a model includes various steps:\r\n",
        "1.  In order to train the model, we need to define the *train_task* and *eval_tax*.  \r\n",
        "2.  Define the training loop using above tasks.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iVbbDj7-eyQ"
      },
      "source": [
        "def train_model(classifier, batch_size, n_steps, output_dir):\r\n",
        "    '''\r\n",
        "    Input: \r\n",
        "        classifier: the model you are building\r\n",
        "        batch_size: the batch size for data-generation\r\n",
        "        n_steps: the evaluation steps\r\n",
        "        output_dir: folder to save your files\r\n",
        "    Output:\r\n",
        "        trainer: trax trainer\r\n",
        "    '''\r\n",
        "    # 1- Define the train_task\r\n",
        "    train_task = training.TrainTask(\r\n",
        "        labeled_data=train_generator(batch_size=batch_size, shuffle=True),\r\n",
        "        loss_layer=tl.CrossEntropyLoss(),\r\n",
        "        optimizer=trax.optimizers.Adam(0.01),\r\n",
        "        n_steps_per_checkpoint=10,\r\n",
        "    )\r\n",
        "\r\n",
        "    # 2- Define the eva_task\r\n",
        "    eval_task = training.EvalTask(\r\n",
        "        labeled_data=val_generator(batch_size=batch_size, shuffle=True),\r\n",
        "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\r\n",
        "    )\r\n",
        "\r\n",
        "    # 3- training loop\r\n",
        "    training_loop = training.Loop(\r\n",
        "                                classifier, # The learning model\r\n",
        "                                train_task, # The training task\r\n",
        "                                eval_task = eval_task, # The evaluation task\r\n",
        "                                output_dir = output_dir) # The output directory\r\n",
        "\r\n",
        "    # 4- run the training loop\r\n",
        "    training_loop.run(n_steps = n_steps)\r\n",
        "\r\n",
        "    # 5- Return the training_loop, since it has the model.\r\n",
        "    return training_loop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcRIGiCaHxfX"
      },
      "source": [
        "## Model and Training Instantiate\r\n",
        "Training Initialization process includes following step:\r\n",
        "1.   Initialization *Batch size* and *Random seed*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyXlCUoZ75Ws"
      },
      "source": [
        "# initialize batch_size, random seed\r\n",
        "batch_size = 16\r\n",
        "rnd.seed(271)\r\n",
        "n_steps = 100\r\n",
        "output_dir=\"./output_dir/\"\r\n",
        "\r\n",
        "# define the model(classifier) with len(cats) output.\r\n",
        "model = classifier(output_dim=len(cats))\r\n",
        "\r\n",
        "# instantiate training \r\n",
        "training_loop = train_model(model, batch_size=batch_size, n_steps=n_steps, output_dir=output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJAW8I-JrJ5p"
      },
      "source": [
        "# Evaluation\r\n",
        "This section describes and implements required steps to evaluate a trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK2TFgnQqNcR"
      },
      "source": [
        " ## Prediction\r\n",
        "This section shows the prediction for a given input consists of following steps:\r\n",
        "1.   Get an input sample using data generator.\r\n",
        "2.   Pass the input to the prediction and get the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_hOkfS2r46Z"
      },
      "source": [
        "# Step 1: get an input sample and check the shapes\r\n",
        "\r\n",
        "# Create a generator object\r\n",
        "tmp_train_generator = train_generator(16)\r\n",
        "\r\n",
        "# get one batch\r\n",
        "tmp_batch = next(tmp_train_generator)\r\n",
        "\r\n",
        "# 0: inputs, 1: targets (the actual labels)\r\n",
        "tmp_inputs, tmp_targets = tmp_batch\r\n",
        "\r\n",
        "# print out the shape of inputs\r\n",
        "print(f'The shape of input = {temp_inputs.shape} & the shape of targes = {temp_targets.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNN2gtpCtfCC"
      },
      "source": [
        "Following code is for the second step of prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IWCWOYVteGb"
      },
      "source": [
        "# Step 2: feed the question tensors into the model to get a prediction\r\n",
        "tmp_pred = training_loop.eval_model(tmp_inputs)\r\n",
        "\r\n",
        "# print out the shape of prediction.\r\n",
        "print(f\"The prediction shape is {tmp_pred.shape}, number of question tensors as rows\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOnrqLukuMk6"
      },
      "source": [
        "## Predicted Class\r\n",
        "Each column represent a category of question. Therefore, it shows the probability that the question belongs to that category. The column with the highest probability is selected as the predicted ouput."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJqew6tx01mW"
      },
      "source": [
        "# get the max between the columns \r\n",
        "\r\n",
        "# get the index and name of question category\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCcmMvC5x4dS"
      },
      "source": [
        "## Evaluation of a batch\r\n",
        "Following method will evaluate the performance of a model on a branch. It will return the *accuracy*, *# of corrects* and *total number*. \r\n",
        "This method will be used later to evaluate the model on the complete *test data*.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3ZOGYvlyh-O"
      },
      "source": [
        "# Evaluate the performance of model on a given branch."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcX5uiHM0eIC"
      },
      "source": [
        "This section uses the *test* data to evaluate the performance of trained model. Therefore, following steps are required:\r\n",
        "\r\n",
        " Define data-generators\r\n",
        "\r\n",
        "*   Define test data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i8k5hcQwTIq"
      },
      "source": [
        "# define data-generators\r\n",
        "test_data_gen = test_generator(batch_size=batch_size, shuffle=False)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}