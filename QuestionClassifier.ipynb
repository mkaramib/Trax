{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuestionClassifier.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMq+HrnqrjoJ1x7gLaSwpej",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkaramib/trax/blob/main/QuestionClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT5D9NVs7jp9"
      },
      "source": [
        "## Question Classification\r\n",
        "In this notebook, I will implement a question classifier using Trax deep learning framework. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXrjGZy9aj2s"
      },
      "source": [
        "import numpy as np_base  # regular ol' numpy\r\n",
        "import os\r\n",
        "import random as rnd\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from unicodedata import normalize\r\n",
        "import re\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXeCE1qqhhEq"
      },
      "source": [
        "# Initialize\r\n",
        "Some of the libraries need to be downlowed or initialized such as NLTK tokenizer and stop-words. Following lines will do these steps. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJqnWqA5hwmu",
        "outputId": "328cdf1a-7296-4a35-bfbf-8783550f626e"
      },
      "source": [
        "# initialize\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH_6OpuCa-GI"
      },
      "source": [
        "# Trax:\r\n",
        "In this section, we need to install [trax](https://github.com/google/trax) if it is not installed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykH9bNfx7iBh"
      },
      "source": [
        "!pip install -q -U trax\r\n",
        "import trax\r\n",
        "from trax import layers as tl  # core building block\r\n",
        "from trax import shapes  # data signatures: dimensionality and type\r\n",
        "from trax import fastmath  # uses jax, offers numpy on steroids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NGCJvW7arR1"
      },
      "source": [
        "Check which version of Trax has been installed.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qT5kXjoaqk8"
      },
      "source": [
        "!pip list | grep trax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS3sJ73AboA7"
      },
      "source": [
        "# Trax Numpy\r\n",
        "The key mathematical benefit of Trax is using JAX to implement its numpy version. So, following line will import Trax' numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-g-O_-zb59u"
      },
      "source": [
        "# import trax.fastmath.numpy\r\n",
        "import trax.fastmath.numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hctu-A1i78ib"
      },
      "source": [
        "## Data \r\n",
        "In this section, all the training and testing questions are read.\r\n",
        "*   Train Data: contains 1000, 2000, 3000, 4000, or 5500 questions in each file.\r\n",
        "*   Test Data: contains close to 500 questions to evaluate the trained model.\r\n",
        "\r\n",
        "In each file(train and test), each line contains a question which has the following format:\r\n",
        "*   QuestionCategory: Question content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUukXdvH80o0"
      },
      "source": [
        "train_f = open(\"./questions/train_1000.label\", mode='r',encoding=\"ISO-8859-1\")\r\n",
        "test_f = open(\"./questions/TREC_10.label\", mode='r', encoding=\"ISO-8859-1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXpxmzTQcTgR"
      },
      "source": [
        "# Tokenization\r\n",
        "One of the key steps in the preprocess is to tokenize the questions. In this experiment, we use NLTK tokenization. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf75I1FniKGu"
      },
      "source": [
        "def tokenize(question):\r\n",
        "    \"\"\"\r\n",
        "    separate the question type as well as question tokens\r\n",
        "    :param question: given question\r\n",
        "    :return: question_category, question_terms\r\n",
        "    \"\"\"\r\n",
        "    colon = question.find(':')            # index of first colon to separate the question category\r\n",
        "    q_cat = question[0:colon]             # get question type\r\n",
        "    content_normalized = normalize('NFKC', question[colon:])  # normalize the content\r\n",
        "    content_normalized = re.sub(\"[^a-zA-Z. ]\", \"\", content_normalized)  # remove non-alphabetic parts of question\r\n",
        "    terms_all = word_tokenize(content_normalized)             # tokenize the content\r\n",
        "\r\n",
        "    # remove the stop words\r\n",
        "    terms = [w for w in terms_all if not w in stop_words]\r\n",
        "    #terms = terms_all\r\n",
        "    return q_cat, terms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EWiTKFHjIKe"
      },
      "source": [
        "# Data Preparation\r\n",
        "In this step, all the question are read, tokenized and stored in list of tuples: *(category, terms)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1fwqiASjg3k"
      },
      "source": [
        "def prepare_data(file):\r\n",
        "  \"\"\"\r\n",
        "  read the lines from the given file and prepare the list of tuples of questions.\r\n",
        "  :param file: given file\r\n",
        "  :return: list of tuples(category, terms)\r\n",
        "  \"\"\"\r\n",
        "  questions = []\r\n",
        "  lines = file.readlines()\r\n",
        "  for line in lines:\r\n",
        "    cat, terms = tokenize(line)\r\n",
        "    question.append((cat, terms))\r\n",
        "  return questions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3dprvM9lCJ-"
      },
      "source": [
        "# Vocabulary and Targets\r\n",
        "We need the vocabulary and targets to train. In this step, we will make them ready. In the future, we need to convert the content(question) to tensor which is list of numbers. So, we need to keep unique id for each term. We have covered this in the vocab dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri1HLlEglYml"
      },
      "source": [
        "def build_vocabulary(questions):\r\n",
        "  \"\"\"\r\n",
        "  Generate the vocabulary from the questions. \r\n",
        "  :param questions: given list of tuples(cat, terms)\r\n",
        "  :return: list of unique categories, vocabulary(dictionary of term:Id)\r\n",
        "  \"\"\"\r\n",
        "  cats = [cat for (cat, _) in questions]\r\n",
        "  vocab = {'__PAD__': 0, '__UNK__': 1}\r\n",
        "  for (_, terms) in questions:\r\n",
        "    for term in terms:\r\n",
        "       if term not in vocab:\r\n",
        "         vocab[term] = len(vocab) \r\n",
        "  return list(set(cats)), vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJv3tObDxPoV"
      },
      "source": [
        "# Build Tensor\r\n",
        "Oen of the first steps in training any neural network is to convert any input to tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SydeK4axht0"
      },
      "source": [
        "def question_to_tensor(question, vocab, unk_token=\"__UNK__\"):\r\n",
        "  \"\"\"\r\n",
        "  convert the given question into tensor\r\n",
        "  :param question: list of terms of question, [t1, t2, ...]\r\n",
        "  :param vocab: dictionary of vocabulary\r\n",
        "  :param unk_token: token to be used for the terms that are not in the vocabs.\r\n",
        "  :return: tensor = [1, 4, 2, ...]\r\n",
        "  \"\"\"\r\n",
        "  tensor = []\r\n",
        "  for term in question:\r\n",
        "    # get the id for the term\r\n",
        "    word_id = vocab[term] if term in vocab else vocab[unk_token]\r\n",
        "    tensor.append(word_id)\r\n",
        "\r\n",
        "  return tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsZQBusK0NVQ"
      },
      "source": [
        "# Batch Data Generator\r\n",
        "In most Deep NN models, the inputs are given in batches. A batch generator is implemented to generate batches of data samples for *train*, *validation*, and *test*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG0yKxQO0oes"
      },
      "source": [
        "def data_generator(data, vocab, cats, batch_size, loop,shuffle=False):\r\n",
        "  '''\r\n",
        "  Generate a batch of samples from the given data.\r\n",
        "  :param data: list of tuples of questions:(cat, terms).\r\n",
        "  :param vocab: vocabulary dictionary {term:id, ...}\r\n",
        "  :param cats: list of categories of questions.\r\n",
        "  :param batch_size: size of batch.\r\n",
        "  :param loop: True/False to loop back at the end of data.\r\n",
        "  :param shuffle: True/False, shuffle the data or not.\r\n",
        "  :Yield: inputs: subset of data samples, target: corresponing targets of selected inputs.\r\n",
        "  '''\r\n",
        "\r\n",
        "  # build a list of indexes for data samples\r\n",
        "  data_l = len(data)\r\n",
        "  data_indexes = list(range(data_l))\r\n",
        "\r\n",
        "  # get the max length of questions for padding. \r\n",
        "  max_l = 0\r\n",
        "  max_l = max(max_l, len(q)) for (_,q) in data\r\n",
        "\r\n",
        "  # shuffle the indexes if it is True\r\n",
        "  if shuffle:\r\n",
        "    rnd.shuffle(data_indexes)\r\n",
        "\r\n",
        "  stop = False\r\n",
        "  index = 0\r\n",
        "\r\n",
        "  while not stop:\r\n",
        "    batch = []\r\n",
        "    targets = []\r\n",
        "    \r\n",
        "    for i in range(batch_size):\r\n",
        "\r\n",
        "        # if at the end of data.\r\n",
        "        if index >= len(data_indexes):\r\n",
        "          if not loop:\r\n",
        "            stop = True\r\n",
        "            break\r\n",
        "          \r\n",
        "          # start index from 0\r\n",
        "          index = 0\r\n",
        "          \r\n",
        "          # shuffle the data indexes if required\r\n",
        "          if shuffle:\r\n",
        "            rnd.shuffle(data_indexes)\r\n",
        "          \r\n",
        "        # get the question, convert to tensor, and append the data and target\r\n",
        "        q = data[data_indexes[index]]\r\n",
        "        q_tensor = question_to_tensor(q[1], vocab)\r\n",
        "        \r\n",
        "        # pad the batched tensors to the longest question in the data.\r\n",
        "        q_tensor_pad = q_tensor + [vocab[\"__PAD__\"]]*(max_l - len(q_tensor)) \r\n",
        "        \r\n",
        "        batch.append(q_tensor)\r\n",
        "        targets.append(cats.index(q[0]))\r\n",
        "\r\n",
        "        # increase index\r\n",
        "        index += 1\r\n",
        "\r\n",
        "    # if stop\r\n",
        "    if stop:\r\n",
        "      break\r\n",
        "\r\n",
        "    # yield the batch and targets\r\n",
        "    yield np.array(batch), np.array(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDUIDW-ZgrAp"
      },
      "source": [
        "We need to build data generators for training, validation, and testing processes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9QFzUnHhHmU"
      },
      "source": [
        "def train_generator(train_f, )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}