{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuestionClassifier.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM5xb5JAPUjFf3wcCzNWLtB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkaramib/trax/blob/main/QuestionClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT5D9NVs7jp9"
      },
      "source": [
        "## Question Classification\r\n",
        "In this notebook, I will implement a question classifier using Trax deep learning framework. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXrjGZy9aj2s"
      },
      "source": [
        "import numpy as np_base  # regular ol' numpy\r\n",
        "import os\r\n",
        "import random as rnd\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from unicodedata import normalize\r\n",
        "import re\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXeCE1qqhhEq"
      },
      "source": [
        "# Initialize\r\n",
        "Some of the libraries need to be downlowed or initialized such as NLTK tokenizer and stop-words. Following lines will do these steps. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJqnWqA5hwmu",
        "outputId": "9dfddff9-7d95-4663-eda4-ce4b5b2e6709"
      },
      "source": [
        "# initialize\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH_6OpuCa-GI"
      },
      "source": [
        "# Trax:\r\n",
        "In this section, we need to install [trax](https://github.com/google/trax) if it is not installed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykH9bNfx7iBh",
        "outputId": "c940e9eb-9368-4bb2-c5eb-7f394d0113a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -q -U trax\r\n",
        "import trax\r\n",
        "from trax import layers as tl  # core building block\r\n",
        "from trax import shapes  # data signatures: dimensionality and type\r\n",
        "from trax import fastmath  # uses jax, offers numpy on steroids\r\n",
        "from trax.supervised import training"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 471kB 6.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.6MB 10.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 38.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4MB 35.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 58.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.7MB 39.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 9.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 348kB 51.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 50.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 52.2MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NGCJvW7arR1"
      },
      "source": [
        "Check which version of Trax has been installed.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qT5kXjoaqk8",
        "outputId": "8bf11103-ba91-42ff-e379-0ef3ce945f50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip list | grep trax"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trax                          1.3.6                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS3sJ73AboA7"
      },
      "source": [
        "## Trax Numpy\r\n",
        "The key mathematical benefit of Trax is using JAX to implement its numpy version. So, following line will import Trax' numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-g-O_-zb59u"
      },
      "source": [
        "# import trax.fastmath.numpy\r\n",
        "import trax.fastmath.numpy as np"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hctu-A1i78ib"
      },
      "source": [
        "# Data \r\n",
        "In this section, all the training and testing questions are read.\r\n",
        "*   Train Data: contains 1000, 2000, 3000, 4000, or 5500 questions in each file.\r\n",
        "*   Validation Data: we use one of training files for validation data.\r\n",
        "*   Test Data: contains close to 500 questions to evaluate the trained model.\r\n",
        "\r\n",
        "In each file(train and test), each line contains a question which has the following format:\r\n",
        "*   QuestionCategory: Question content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUukXdvH80o0"
      },
      "source": [
        "train_file = open(\"./questions/train_2000.label\", mode='r',encoding=\"ISO-8859-1\")\r\n",
        "val_file = open(\"./questions/train_1000.label\", mode='r',encoding=\"ISO-8859-1\")\r\n",
        "test_file = open(\"./questions/TREC_10.label\", mode='r', encoding=\"ISO-8859-1\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXpxmzTQcTgR"
      },
      "source": [
        "## Tokenization\r\n",
        "One of the key steps in the preprocess is to tokenize the questions. In this experiment, we use NLTK tokenization. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf75I1FniKGu"
      },
      "source": [
        "def tokenize(question):\r\n",
        "    \"\"\"\r\n",
        "    separate the question type as well as question tokens\r\n",
        "    :param question: given question\r\n",
        "    :return: question_category, question_terms\r\n",
        "    \"\"\"\r\n",
        "    colon = question.find(':')            # index of first colon to separate the question category\r\n",
        "    q_cat = question[0:colon]             # get question type\r\n",
        "    content_normalized = normalize('NFKC', question[colon:])  # normalize the content\r\n",
        "    content_normalized = re.sub(\"[^a-zA-Z. ]\", \"\", content_normalized)  # remove non-alphabetic parts of question\r\n",
        "    terms_all = word_tokenize(content_normalized)             # tokenize the content\r\n",
        "\r\n",
        "    # remove the stop words\r\n",
        "    terms = [w for w in terms_all if not w in stop_words]\r\n",
        "    #terms = terms_all\r\n",
        "    return q_cat, terms"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EWiTKFHjIKe"
      },
      "source": [
        "## Data Preprocess\r\n",
        "In this step, all the question are read, tokenized and stored in list of tuples: *(category, terms)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1fwqiASjg3k"
      },
      "source": [
        "def load_data(file):\r\n",
        "  \"\"\"\r\n",
        "  read the lines from the given file and prepare the list of tuples of questions.\r\n",
        "  :param file: given file\r\n",
        "  :return: list of tuples(category, terms)\r\n",
        "  \"\"\"\r\n",
        "  questions = []\r\n",
        "  lines = file.readlines()\r\n",
        "  for line in lines:\r\n",
        "    cat, terms = tokenize(line)\r\n",
        "    questions.append((cat, terms))\r\n",
        "  return questions"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3dprvM9lCJ-"
      },
      "source": [
        "# Vocabulary and Targets\r\n",
        "We need the vocabulary and targets to train. In this step, we will make them ready. In the future, we need to convert the content(question) to tensor which is list of numbers. So, we need to keep unique id for each term. We have covered this in the vocab dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri1HLlEglYml"
      },
      "source": [
        "def build_vocabulary(questions):\r\n",
        "  \"\"\"\r\n",
        "  Generate the vocabulary from the questions. \r\n",
        "  :param questions: given list of tuples(cat, terms)\r\n",
        "  :return: list of unique categories, vocabulary(dictionary of term:Id)\r\n",
        "  \"\"\"\r\n",
        "  cats = [cat for (cat, _) in questions]\r\n",
        "  vocab = {'__PAD__': 0, '__UNK__': 1}\r\n",
        "  for (_, terms) in questions:\r\n",
        "    for term in terms:\r\n",
        "       if term not in vocab:\r\n",
        "         vocab[term] = len(vocab) \r\n",
        "  return list(set(cats)), vocab"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJv3tObDxPoV"
      },
      "source": [
        "# Build Tensor\r\n",
        "Oen of the first steps in training any neural network is to convert any input to tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SydeK4axht0"
      },
      "source": [
        "def question_to_tensor(question, vocab, unk_token=\"__UNK__\"):\r\n",
        "  \"\"\"\r\n",
        "  convert the given question into tensor\r\n",
        "  :param question: list of terms of question, [t1, t2, ...]\r\n",
        "  :param vocab: dictionary of vocabulary\r\n",
        "  :param unk_token: token to be used for the terms that are not in the vocabs.\r\n",
        "  :return: tensor = [1, 4, 2, ...]\r\n",
        "  \"\"\"\r\n",
        "  tensor = []\r\n",
        "  for term in question:\r\n",
        "    # get the id for the term\r\n",
        "    word_id = vocab[term] if term in vocab else vocab[unk_token]\r\n",
        "    tensor.append(word_id)\r\n",
        "\r\n",
        "  return tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsZQBusK0NVQ"
      },
      "source": [
        "# Batch Data Generator\r\n",
        "In most Deep NN models, the inputs are given in batches. A batch generator is implemented to generate batches of data samples for *train*, *validation*, and *test*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG0yKxQO0oes"
      },
      "source": [
        "def data_generator(data, vocab, cats, batch_size, loop, shuffle=False):\r\n",
        "  '''\r\n",
        "  Generate a batch of samples from the given data.\r\n",
        "  :param data: list of tuples of questions:(cat, terms).\r\n",
        "  :param vocab: vocabulary dictionary {term:id, ...}\r\n",
        "  :param cats: list of categories of questions.\r\n",
        "  :param batch_size: size of batch.\r\n",
        "  :param loop: True/False to loop back at the end of data.\r\n",
        "  :param shuffle: True/False, shuffle the data or not.\r\n",
        "  :Yield: inputs: subset of data samples, target: corresponing targets of selected inputs.\r\n",
        "  '''\r\n",
        "\r\n",
        "  # build a list of indexes for data samples\r\n",
        "  data_l = len(data)\r\n",
        "  data_indexes = list(range(data_l))\r\n",
        "\r\n",
        "  # get the max length of questions for padding. \r\n",
        "  max_l = 0\r\n",
        "  max_l = max(max_l, len(q)) for (_,q) in data\r\n",
        "\r\n",
        "  # shuffle the indexes if it is True\r\n",
        "  if shuffle:\r\n",
        "    rnd.shuffle(data_indexes)\r\n",
        "\r\n",
        "  stop = False\r\n",
        "  index = 0\r\n",
        "\r\n",
        "  while not stop:\r\n",
        "    batch = []\r\n",
        "    targets = []\r\n",
        "    \r\n",
        "    for i in range(batch_size):\r\n",
        "\r\n",
        "        # if at the end of data.\r\n",
        "        if index >= len(data_indexes):\r\n",
        "          if not loop:\r\n",
        "            stop = True\r\n",
        "            break\r\n",
        "          \r\n",
        "          # start index from 0\r\n",
        "          index = 0\r\n",
        "          \r\n",
        "          # shuffle the data indexes if required\r\n",
        "          if shuffle:\r\n",
        "            rnd.shuffle(data_indexes)\r\n",
        "          \r\n",
        "        # get the question, convert to tensor, and append the data and target\r\n",
        "        q = data[data_indexes[index]]\r\n",
        "        q_tensor = question_to_tensor(q[1], vocab)\r\n",
        "        \r\n",
        "        # pad the batched tensors to the longest question in the data.\r\n",
        "        q_tensor_pad = q_tensor + [vocab[\"__PAD__\"]]*(max_l - len(q_tensor)) \r\n",
        "        \r\n",
        "        batch.append(q_tensor)\r\n",
        "        targets.append(cats.index(q[0]))\r\n",
        "\r\n",
        "        # increase index\r\n",
        "        index += 1\r\n",
        "\r\n",
        "    # if stop\r\n",
        "    if stop:\r\n",
        "      break\r\n",
        "\r\n",
        "    # yield the batch and targets\r\n",
        "    yield np.array(batch), np.array(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDUIDW-ZgrAp"
      },
      "source": [
        "We need to build data generators for training, validation, and testing processes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9QFzUnHhHmU"
      },
      "source": [
        "def train_generator(train_questions,shuffle=False):\r\n",
        "  return data_generator(train_questions, vocab, cats, batch_size, True,shuffle=False)\r\n",
        "\r\n",
        "def eval_generator(eval_questions, shuffle=False):\r\n",
        "  return data_generator(eval_questions, vocab, cats, batch_size, True,shuffle=False)\r\n",
        "\r\n",
        "def test_generator(test_questions, shuffle=False):\r\n",
        "  return data_generator(test_questions, vocab, cats, batch_size, True,shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWz1k5PD6hz4"
      },
      "source": [
        "# Classifier\r\n",
        "Classifier is a function that builds a neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn7hNf3B6u6x"
      },
      "source": [
        "def classifier(vocab_size=len(Vocab), embedding_dim=256, output_dim=2, mode='train'):\r\n",
        "    # create embedding layer\r\n",
        "    embed_layer = tl.Embedding(\r\n",
        "        vocab_size = vocab_size,    # Size of the vocabulary\r\n",
        "        d_feature = embedding_dim)  # Embedding dimension\r\n",
        "    \r\n",
        "    # Create a mean layer, to create an \"average\" word embedding\r\n",
        "    mean_layer = tl.Mean(axis=1)\r\n",
        "    \r\n",
        "    # Create a dense layer, one unit for each output\r\n",
        "    dense_output_layer = tl.Dense(n_units = output_dim)\r\n",
        "    \r\n",
        "    # Create the log softmax layer.\r\n",
        "    log_softmax_layer = tl.LogSoftmax()\r\n",
        "    \r\n",
        "    # Create a model using tl.Serial to combine all layers\r\n",
        "    model = tl.Serial(\r\n",
        "      embed_layer,        # embedding layer\r\n",
        "      mean_layer,         # mean layer\r\n",
        "      dense_output_layer, # dense output layer \r\n",
        "      log_softmax_layer   # log softmax layer\r\n",
        "    )\r\n",
        "\r\n",
        "    # return the model of type\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Eax3EJ7v1a"
      },
      "source": [
        "# Train\r\n",
        "Training a model includes various steps:\r\n",
        "1.  In order to train the model, we need to define the *train_task* and *eval_tax*.  \r\n",
        "2.  Define the training loop using above tasks.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iVbbDj7-eyQ"
      },
      "source": [
        "def train_model(classifier, batch_size, n_steps, output_dir):\r\n",
        "    '''\r\n",
        "    Input: \r\n",
        "        classifier: the model you are building\r\n",
        "        batch_size: the batch size for data-generation\r\n",
        "        n_steps: the evaluation steps\r\n",
        "        output_dir: folder to save your files\r\n",
        "    Output:\r\n",
        "        trainer: trax trainer\r\n",
        "    '''\r\n",
        "    # 1- Define the train_task\r\n",
        "    train_task = training.TrainTask(\r\n",
        "        labeled_data=train_generator(batch_size=batch_size, shuffle=True),\r\n",
        "        loss_layer=tl.CrossEntropyLoss(),\r\n",
        "        optimizer=trax.optimizers.Adam(0.01),\r\n",
        "        n_steps_per_checkpoint=10,\r\n",
        "    )\r\n",
        "\r\n",
        "    # 2- Define the eva_task\r\n",
        "    eval_task = training.EvalTask(\r\n",
        "        labeled_data=val_generator(batch_size=batch_size, shuffle=True),\r\n",
        "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\r\n",
        "    )\r\n",
        "\r\n",
        "    # 3- training loop\r\n",
        "    training_loop = training.Loop(\r\n",
        "                                classifier, # The learning model\r\n",
        "                                train_task, # The training task\r\n",
        "                                eval_task = eval_task, # The evaluation task\r\n",
        "                                output_dir = output_dir) # The output directory\r\n",
        "\r\n",
        "    # 4- run the training loop\r\n",
        "    training_loop.run(n_steps = n_steps)\r\n",
        "\r\n",
        "    # 5- Return the training_loop, since it has the model.\r\n",
        "    return training_loop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcRIGiCaHxfX"
      },
      "source": [
        "# Main Function\r\n",
        "Main process includes following steps:\r\n",
        "1.   Initialization\r\n",
        "  *   Batch size\r\n",
        "  *   Random seed\r\n",
        "2.   Load data-sets\r\n",
        "  *   define files\r\n",
        "  *   load data samples\r\n",
        "3.   Build vocabulary (terms and question categories)\r\n",
        "4.   Define data-generators\r\n",
        "  *   test data-generator (other generators are defined in the corresponding tasks)\r\n",
        "5.   \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyXlCUoZ75Ws"
      },
      "source": [
        "# initialize batch_size, random seed\r\n",
        "batch_size = 16\r\n",
        "rnd.seed(271)\r\n",
        "n_steps = 100\r\n",
        "output_dir=\"./output_dir/\"\r\n",
        "\r\n",
        "# load training questions\r\n",
        "training_data = load_data(train_file)\r\n",
        "\r\n",
        "# load validation questions\r\n",
        "validation_data = load_data(val_file)\r\n",
        "\r\n",
        "# load test questions\r\n",
        "test_data = load_data(test_file)\r\n",
        "\r\n",
        "# build vocabulary\r\n",
        "cats, vocab = build_vocabulary(training_data)\r\n",
        "\r\n",
        "# define data-generators\r\n",
        "test_data_gen = test_generator(batch_size=batch_size, shuffle=False)\r\n",
        "\r\n",
        "# define the model(classifier)\r\n",
        "#model = classifier()\r\n",
        "\r\n",
        "# instantiate training \r\n",
        "#training_loop = train_model(model, batch_size=batch_size, n_steps=n_steps, output_dir=output_dir)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}