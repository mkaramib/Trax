{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuestionClassifier.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMAnKXFw291ewn3HowruEnm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkaramib/trax/blob/main/QuestionClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT5D9NVs7jp9"
      },
      "source": [
        "## Question Classification\r\n",
        "In this notebook, I will implement a question classifier using Trax deep learning framework. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXrjGZy9aj2s"
      },
      "source": [
        "import numpy as np_base  # regular numpy\r\n",
        "import os\r\n",
        "import random as rnd\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from unicodedata import normalize\r\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXeCE1qqhhEq"
      },
      "source": [
        "# Initialize\r\n",
        "Some of the libraries need to be downlowed or initialized such as NLTK tokenizer and stop-words. Following lines will do these steps. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJqnWqA5hwmu"
      },
      "source": [
        "# initialize\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH_6OpuCa-GI"
      },
      "source": [
        "# Trax\r\n",
        "In this section, we need to install [trax](https://github.com/google/trax) if it is not installed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykH9bNfx7iBh"
      },
      "source": [
        "!pip install -q -U trax\r\n",
        "import trax\r\n",
        "from trax import layers as tl  # core building block\r\n",
        "from trax import shapes  # data signatures: dimensionality and type\r\n",
        "from trax import fastmath  # uses jax, offers numpy on steroids\r\n",
        "from trax.supervised import training"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NGCJvW7arR1"
      },
      "source": [
        "Check which version of Trax has been installed.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qT5kXjoaqk8"
      },
      "source": [
        "!pip list | grep trax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS3sJ73AboA7"
      },
      "source": [
        "## Trax Numpy\r\n",
        "The key mathematical benefit of Trax is using JAX to implement its numpy version. So, following line will import Trax' numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-g-O_-zb59u"
      },
      "source": [
        "# import trax.fastmath.numpy\r\n",
        "import trax.fastmath.numpy as np"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hctu-A1i78ib"
      },
      "source": [
        "# Data Processing\r\n",
        "In this section, all the training and testing questions are read.\r\n",
        "*   Train Data: contains 1000, 2000, 3000, 4000, or 5500 questions in each file.\r\n",
        "*   Validation Data: we use one of training files for validation data.\r\n",
        "*   Test Data: contains close to 500 questions to evaluate the trained model.\r\n",
        "\r\n",
        "In each file(train and test), each line contains a question which has the following format:\r\n",
        "*   QuestionCategory: Question content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUukXdvH80o0"
      },
      "source": [
        "train_file = open(\"./questions/train_4000.label\", mode='r',encoding=\"ISO-8859-1\")\r\n",
        "val_file = open(\"./questions/train_1000.label\", mode='r',encoding=\"ISO-8859-1\")\r\n",
        "test_file = open(\"./questions/TREC_10.label\", mode='r', encoding=\"ISO-8859-1\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXpxmzTQcTgR"
      },
      "source": [
        "## Tokenization\r\n",
        "One of the key steps in the preprocess is to tokenize the questions. In this experiment, we use NLTK tokenization. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf75I1FniKGu"
      },
      "source": [
        "def tokenize(question):\r\n",
        "    \"\"\"\r\n",
        "    separate the question type as well as question tokens\r\n",
        "    :param question: given question\r\n",
        "    :return: question_category, question_terms\r\n",
        "    \"\"\"\r\n",
        "    colon = question.find(':')            # index of first colon to separate the question category\r\n",
        "    q_cat = question[0:colon]             # get question type\r\n",
        "    content_normalized = normalize('NFKC', question[colon:])  # normalize the content\r\n",
        "    content_normalized = re.sub(\"[^a-zA-Z. ]\", \"\", content_normalized)  # remove non-alphabetic parts of question\r\n",
        "    terms_all = word_tokenize(content_normalized)             # tokenize the content\r\n",
        "\r\n",
        "    # remove the stop words\r\n",
        "    #terms = [w for w in terms_all if w not in stop_words]\r\n",
        "    terms = terms_all\r\n",
        "    return q_cat, terms"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EWiTKFHjIKe"
      },
      "source": [
        "## Data Preprocess\r\n",
        "In this step, all the question are read, tokenized and stored in list of tuples: *(category, terms)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1fwqiASjg3k"
      },
      "source": [
        "def load_data(file):\r\n",
        "  \"\"\"\r\n",
        "  read the lines from the given file and prepare the list of tuples of questions.\r\n",
        "  :param file: given file\r\n",
        "  :return: list of tuples(category, terms)\r\n",
        "  \"\"\"\r\n",
        "  questions = []\r\n",
        "  lines = file.readlines()\r\n",
        "  for line in lines:\r\n",
        "    cat, terms = tokenize(line)\r\n",
        "    questions.append((cat, terms))\r\n",
        "  return questions"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3dprvM9lCJ-"
      },
      "source": [
        "## Vocabulary and Targets\r\n",
        "We need the vocabulary and targets to train. In this step, we will make them ready. In the future, we need to convert the content(question) to tensor which is list of numbers. So, we need to keep unique id for each term. We have covered this in the vocab dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri1HLlEglYml"
      },
      "source": [
        "def build_vocabulary(questions):\r\n",
        "  \"\"\"\r\n",
        "  Generate the vocabulary from the questions. \r\n",
        "  :param questions: given list of tuples(cat, terms)\r\n",
        "  :return: list of unique categories, vocabulary(dictionary of term:Id)\r\n",
        "  \"\"\"\r\n",
        "  cats = [cat for (cat, _) in questions]\r\n",
        "  vocab = {'__PAD__': 0, '__UNK__': 1}\r\n",
        "  for (_, terms) in questions:\r\n",
        "    for term in terms:\r\n",
        "       if term not in vocab:\r\n",
        "         vocab[term] = len(vocab) \r\n",
        "  return list(set(cats)), vocab"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCV4P3OfZw1B"
      },
      "source": [
        "#### Vocabulary Test\r\n",
        "Do not run the next piece of code if you want to run the whole system at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X-IsDLrwISE"
      },
      "source": [
        "# test the vocabulary builder\r\n",
        "temp_cats, temp_vocabs = build_vocabulary(temp_train_qs)\r\n",
        "print(f'Categories = {temp_cats}')\r\n",
        "print(f'number of terms in vocab = {len(temp_vocabs)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJv3tObDxPoV"
      },
      "source": [
        "## Build Tensor\r\n",
        "Oen of the first steps in training any neural network is to convert any input to tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SydeK4axht0"
      },
      "source": [
        "def question_to_tensor(question, vocab, unk_token=\"__UNK__\"):\r\n",
        "  \"\"\"\r\n",
        "  convert the given question into tensor\r\n",
        "  :param question: list of terms of question, [t1, t2, ...]\r\n",
        "  :param vocab: dictionary of vocabulary\r\n",
        "  :param unk_token: token to be used for the terms that are not in the vocabs.\r\n",
        "  :return: tensor = [1, 4, 2, ...]\r\n",
        "  \"\"\"\r\n",
        "  tensor = []\r\n",
        "  for term in question:\r\n",
        "    # get the id for the term\r\n",
        "    word_id = vocab[term] if term in vocab else vocab[unk_token]\r\n",
        "    tensor.append(word_id)\r\n",
        "\r\n",
        "  return tensor"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POyzIu5fZ90O"
      },
      "source": [
        "#### Tensor Test\r\n",
        "Test how is the results of building tensor for a given question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PnYQsW9xAmM"
      },
      "source": [
        "# test question_to_tensor builder\r\n",
        "temp_tensor = question_to_tensor(temp_train_qs[1][1],temp_vocabs)\r\n",
        "print(f'question_terms = {temp_train_qs[1][1]}, tensor = {temp_tensor}, shape = {len(temp_tensor)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouohzLWtkuGB"
      },
      "source": [
        "## Data Loader\r\n",
        "It is required to load *train*, *validation*, and *test* data and collect the vocabulary and question categories. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqOWdh4blB-s"
      },
      "source": [
        "# load training questions\r\n",
        "training_data = load_data(train_file)\r\n",
        "\r\n",
        "# load validation questions\r\n",
        "validation_data = load_data(val_file)\r\n",
        "\r\n",
        "# load test questions\r\n",
        "test_data = load_data(test_file)\r\n",
        "\r\n",
        "# build vocabulary\r\n",
        "cats, vocab = build_vocabulary(training_data)\r\n",
        "\r\n",
        "print(f'training size = {len(training_data)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsZQBusK0NVQ"
      },
      "source": [
        "## Data Generator(Batch)\r\n",
        "In most Deep NN models, the inputs are given in batches. A batch generator is implemented to generate batches of data samples for *train*, *validation*, and *test*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG0yKxQO0oes"
      },
      "source": [
        "def data_generator(data, vocab, cats, batch_size, loop=False, shuffle=False):\r\n",
        "  '''\r\n",
        "  Generate a batch of samples from the given data.\r\n",
        "  :param data: list of tuples of questions:(cat, terms).\r\n",
        "  :param vocab: vocabulary dictionary {term:id, ...}\r\n",
        "  :param cats: list of categories of questions.\r\n",
        "  :param batch_size: size of batch.\r\n",
        "  :param loop: True/False to loop back at the end of data.\r\n",
        "  :param shuffle: True/False, shuffle the data or not.\r\n",
        "  :Yield: inputs: subset of data samples, target: corresponing targets of selected inputs.\r\n",
        "  '''\r\n",
        "\r\n",
        "  # build a list of indexes for data samples\r\n",
        "  data_l = len(data)\r\n",
        "  data_indexes = list(range(data_l))\r\n",
        "\r\n",
        "  # get the max length of questions for padding. \r\n",
        "  max_l = 0\r\n",
        "  for (_,q) in data:\r\n",
        "    max_l = max(max_l, len(q))\r\n",
        "\r\n",
        "  # shuffle the indexes if it is True\r\n",
        "  if shuffle:\r\n",
        "    rnd.shuffle(data_indexes)\r\n",
        "\r\n",
        "  stop = False\r\n",
        "  index = 0\r\n",
        "\r\n",
        "  while not stop:\r\n",
        "    batch = []\r\n",
        "    targets = []\r\n",
        "    \r\n",
        "    for i in range(batch_size):\r\n",
        "\r\n",
        "        # if at the end of data.\r\n",
        "        if index >= len(data_indexes):\r\n",
        "          if not loop:\r\n",
        "            stop = True\r\n",
        "            break\r\n",
        "          \r\n",
        "          # start index from 0\r\n",
        "          index = 0\r\n",
        "          \r\n",
        "          # shuffle the data indexes if required\r\n",
        "          if shuffle:\r\n",
        "            rnd.shuffle(data_indexes)\r\n",
        "          \r\n",
        "        # get the question, convert to tensor, and append the data and target\r\n",
        "        q = data[data_indexes[index]]\r\n",
        "        q_tensor = question_to_tensor(q[1], vocab)\r\n",
        "        \r\n",
        "        # pad the batched tensors to the longest question in the data.\r\n",
        "        q_tensor_pad = q_tensor + [vocab[\"__PAD__\"]]*(max_l - len(q_tensor)) \r\n",
        "        \r\n",
        "        batch.append(q_tensor_pad)\r\n",
        "        targets.append(cats.index(q[0]))\r\n",
        "\r\n",
        "        # increase index\r\n",
        "        index += 1\r\n",
        "\r\n",
        "    # generate weights, treat all the questions same. \r\n",
        "    weights = np.ones_like(np.arange(batch_size))\r\n",
        "\r\n",
        "    # yield the batch and targets\r\n",
        "    yield np.array(batch), np.array(targets), weights\r\n",
        "\r\n",
        "    # if stop\r\n",
        "    if stop:\r\n",
        "      break"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phKZGzhifJj6"
      },
      "source": [
        "#### Test Data Generator\r\n",
        "Test the result of data generation method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kfxAfJPyWcI"
      },
      "source": [
        "# test the data_generator\r\n",
        "temp_data_generator = data_generator(validation_data, vocab, cats, batch_size=8,loop=False)\r\n",
        "temp_next = next(temp_data_generator)\r\n",
        "print(f'batch shape = {temp_next[0]}, targets = {temp_next[1]}, weights={temp_next[2]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDUIDW-ZgrAp"
      },
      "source": [
        "We need to build data generators for training, validation, and testing processes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9QFzUnHhHmU"
      },
      "source": [
        "def train_generator(batch_size, shuffle=False):\r\n",
        "  return data_generator(training_data, vocab, cats, batch_size, loop=False,shuffle=False)\r\n",
        "\r\n",
        "def val_generator(batch_size, shuffle=False):\r\n",
        "  return data_generator(validation_data, vocab, cats, batch_size, loop=False ,shuffle=False)\r\n",
        "\r\n",
        "def test_generator(batch_size, shuffle=False):\r\n",
        "  return data_generator(test_data, vocab, cats, batch_size, loop=False,shuffle=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xkkVXRGy8RO"
      },
      "source": [
        "# Training\r\n",
        "Training consists of following steps:\r\n",
        "1.   Define the NN model\r\n",
        "2.   Define the training model\r\n",
        "3.   Instantiate the model and training loop\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWz1k5PD6hz4"
      },
      "source": [
        "## NN Model\r\n",
        "Classifier is a function that design the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn7hNf3B6u6x"
      },
      "source": [
        "def classifier(vocab_size=len(vocab), embedding_dim=256, output_dim=2, mode='train'):\r\n",
        "    # create embedding layer\r\n",
        "    embed_layer = tl.Embedding(\r\n",
        "        vocab_size = vocab_size,    # Size of the vocabulary\r\n",
        "        d_feature = embedding_dim)  # Embedding dimension\r\n",
        "    \r\n",
        "    # Create a mean layer, to create an \"average\" word embedding\r\n",
        "    mean_layer = tl.Mean(axis=1)\r\n",
        "\r\n",
        "    # Create a dense layer, one unit for each output\r\n",
        "    dense_output_layer = tl.Dense(n_units = output_dim)\r\n",
        "    \r\n",
        "    # Create the log softmax layer.\r\n",
        "    log_softmax_layer = tl.LogSoftmax()\r\n",
        "    \r\n",
        "    # Create a model using tl.Serial to combine all layers\r\n",
        "    model = tl.Serial(\r\n",
        "      embed_layer,        # embedding layer\r\n",
        "      mean_layer,         # mean layer\r\n",
        "      dense_output_layer, # dense output layer \r\n",
        "      log_softmax_layer   # log softmax layer\r\n",
        "    )\r\n",
        "\r\n",
        "    # return the model of type\r\n",
        "    return model"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cF-v986oQvL"
      },
      "source": [
        "## Train and Evaluation Tasks\r\n",
        "In this section, training and evaluation tasks are built."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqiIBUiDoUcg"
      },
      "source": [
        "# initialize batch_size, random seed\r\n",
        "batch_size = 16\r\n",
        "rnd.seed(271)\r\n",
        "\r\n",
        "# 1- Define the train_task\r\n",
        "train_task = training.TrainTask(\r\n",
        "    labeled_data=train_generator(batch_size=batch_size, shuffle=True),\r\n",
        "    loss_layer=tl.CrossEntropyLoss(),\r\n",
        "    optimizer=trax.optimizers.Adam(0.01),\r\n",
        "    n_steps_per_checkpoint=10,\r\n",
        ")\r\n",
        "\r\n",
        "# 2- Define the eva_task\r\n",
        "eval_task = training.EvalTask(\r\n",
        "    labeled_data=val_generator(batch_size=batch_size, shuffle=True),\r\n",
        "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\r\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Eax3EJ7v1a"
      },
      "source": [
        "## Training Model\r\n",
        "Training a model needs:\r\n",
        "\r\n",
        "*   *training task*\r\n",
        "*   *evaluation task*\r\n",
        "\r\n",
        "and it define and returns the *training loop* using above tasks.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iVbbDj7-eyQ"
      },
      "source": [
        "def train_model(classifier, train_task, eval_task, n_steps, output_dir):\r\n",
        "    '''\r\n",
        "    Input: \r\n",
        "        classifier: the model you are building\r\n",
        "        batch_size: the batch size for data-generation\r\n",
        "        n_steps: the evaluation steps\r\n",
        "        output_dir: folder to save your files\r\n",
        "    Output:\r\n",
        "        trainer: trax trainer\r\n",
        "    '''\r\n",
        "\r\n",
        "    # 3- training loop\r\n",
        "    training_loop = training.Loop(\r\n",
        "                                classifier, # The learning model\r\n",
        "                                train_task, # The training task\r\n",
        "                                #eval_task=eval_task, # The evaluation task\r\n",
        "                                output_dir=output_dir) # The output directory\r\n",
        "\r\n",
        "    # 4- run the training loop\r\n",
        "    training_loop.run(n_steps = n_steps)\r\n",
        "\r\n",
        "    # 5- Return the training_loop, since it has the model.\r\n",
        "    return training_loop"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcRIGiCaHxfX"
      },
      "source": [
        "## Model and Training Instantiate\r\n",
        "Training Initialization process includes following step:\r\n",
        "1.   Initialization *Batch size* and *Random seed*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyXlCUoZ75Ws"
      },
      "source": [
        "n_steps = 200\r\n",
        "output_dir=\"./output_dir/\"\r\n",
        "\r\n",
        "# Remove old model.pkl if it exists\r\n",
        "!rm -f 'model/model.pkl.gz' \r\n",
        "\r\n",
        "# define the model(classifier) with len(cats) output.\r\n",
        "model = classifier(output_dim=len(cats))\r\n",
        "print(model)\r\n",
        "\r\n",
        "# instantiate training process.\r\n",
        "training_loop = train_model(model, train_task, eval_task, n_steps=n_steps, output_dir=output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJAW8I-JrJ5p"
      },
      "source": [
        "# Evaluation\r\n",
        "This section describes and implements required steps to evaluate a trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK2TFgnQqNcR"
      },
      "source": [
        " ## Prediction\r\n",
        "This section shows the prediction for a given input consists of the following steps:\r\n",
        "1.   Get an input sample using data generator.\r\n",
        "2.   Pass the input to the prediction and get the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_hOkfS2r46Z"
      },
      "source": [
        "# Step 1: get an input sample and check the shapes\r\n",
        "\r\n",
        "# Create a generator object\r\n",
        "tmp_test_generator = test_generator(batch_size=16)\r\n",
        "\r\n",
        "# get one batch\r\n",
        "tmp_batch = next(tmp_test_generator)\r\n",
        "\r\n",
        "# 0: inputs, 1: targets (the actual labels)\r\n",
        "tmp_inputs, tmp_targets, tmp_weights = tmp_batch\r\n",
        "\r\n",
        "# print out the shape of inputs\r\n",
        "print(f'The shape of input = {tmp_inputs.shape} & the shape of targes = {tmp_targets.shape}, shape of weights = {tmp_weights.shape}')\r\n",
        "\r\n",
        "# print out one sample of test\r\n",
        "print(f'The test question = {tmp_inputs[0]}, target = {tmp_targets[0]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNN2gtpCtfCC"
      },
      "source": [
        "Following code is for the second step of prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IWCWOYVteGb"
      },
      "source": [
        "# Step 2: feed the question tensors into the model to get a prediction\r\n",
        "tmp_pred = training_loop.eval_model(tmp_inputs)\r\n",
        "\r\n",
        "# print out the shape of prediction.\r\n",
        "print(f\"The prediction shape is {tmp_pred.shape}\")\r\n",
        "\r\n",
        "# print out the prediction\r\n",
        "print(f'The target = {tmp_targets[1]} and The prediction = {np.argmax(tmp_pred[1])}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCcmMvC5x4dS"
      },
      "source": [
        "## Evaluation of a batch\r\n",
        "Following method will evaluate the performance of a model on a batch. It will return the *accuracy*, *# of corrects* and *total number*. \r\n",
        "This method will be used later to evaluate the model on the complete *test data*.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3ZOGYvlyh-O"
      },
      "source": [
        "# Evaluate the performance of model on a given branch.\r\n",
        "def compute_accuracy(predictions, actuals):\r\n",
        "  '''\r\n",
        "  Inputs:\r\n",
        "    predictions: list of list of log softmax\r\n",
        "    actuals: list of actual output for each rows of prediction\r\n",
        "  Output:\r\n",
        "    correct: number of correct predictions\r\n",
        "    total: total number of predictions\r\n",
        "  '''\r\n",
        "\r\n",
        "  # get the max element of each row of predictions\r\n",
        "  predicted_max = np.argmax(predictions, axis=1)\r\n",
        "\r\n",
        "  # calculates corrects \r\n",
        "  corrects = predicted_max == actuals\r\n",
        "\r\n",
        "  # return number of corrects and totals\r\n",
        "  return np.sum(corrects), len(actuals)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcX5uiHM0eIC"
      },
      "source": [
        "## Test the Model\r\n",
        "This section uses the *test* data to evaluate the performance of trained model.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJjWRvVO_RFl"
      },
      "source": [
        "def test_model(generator, model):\r\n",
        "  '''\r\n",
        "  Inputs:\r\n",
        "    generator: data generator which yields batches of test data\r\n",
        "    model: trained model\r\n",
        "\r\n",
        "  Output:\r\n",
        "    accuracy: accuracy of trained model\r\n",
        "  '''\r\n",
        "  \r\n",
        "  accuracy = 0.\r\n",
        "  total_corrects = 0\r\n",
        "  total_preds = 0\r\n",
        "  i = 0\r\n",
        "\r\n",
        "  # iterate through the data generator\r\n",
        "  for batch in generator:\r\n",
        "\r\n",
        "    # get the batches outputs\r\n",
        "    inputs, targets, target_weighs = batch\r\n",
        "\r\n",
        "    # Make predictions using the inputs\r\n",
        "    preds = training_loop.eval_model(inputs)\r\n",
        "\r\n",
        "    # get the batch accuracy\r\n",
        "    batch_corrects, batch_totals = compute_accuracy(preds, targets)\r\n",
        "    print(f'batch {i} , corrects = {batch_corrects}, total = {batch_totals}')\r\n",
        "\r\n",
        "    total_corrects += batch_corrects\r\n",
        "    total_preds += batch_totals\r\n",
        "\r\n",
        "    i += 1\r\n",
        "\r\n",
        "  # calculate the accuracy\r\n",
        "  accuracy = total_corrects / total_preds\r\n",
        "  \r\n",
        "  # return the accuracy\r\n",
        "  return accuracy"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms2WFfC1Dgas"
      },
      "source": [
        "## Final Step\r\n",
        "The final step is to call the *test_model* method sending the test data generator. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i8k5hcQwTIq"
      },
      "source": [
        "# define data-generators\r\n",
        "test_data_gen = test_generator(batch_size=batch_size, shuffle=False)\r\n",
        "\r\n",
        "# define the model\r\n",
        "model = training_loop.eval_model\r\n",
        "\r\n",
        "# call the test_model and get the accuracy\r\n",
        "accuracy = test_model(test_data_gen, model)\r\n",
        "print(f'Accuracy = {accuracy}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}